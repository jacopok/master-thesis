\documentclass[main.tex]{subfiles}
\begin{document}

\section{MLGW-BNS}

Let us start with an overview of the algorithm employed by \ac{mb} before getting into the technical details. 

We start with two systems for the generation of a theoretical waveform from a \ac{CBC} in the frequency domain, one being faster but less accurate than the other. 
The ``fast'' system will be the \ac{PN} model \texttt{TaylorF2}, as implemented in \texttt{bajes} \cite[]{breschiTtBajesBayesian2021}, while the ``slow'' system will be the \ac{EOB} model \texttt{TEOBResumS}, with waveforms generated in the time domain and then Fourier-transformed.

As we shall see shortly, a crucial feature of the ``fast'' system is the capability to generate waveforms in the frequency domain at select frequencies, without needing to perform a full Fourier transform from a time-domain waveform.
This feature is shared by all \ac{PN} approximants, which base their Fourier-domain representations on the analytic \ac{SPA}.

The main idea is then to 
\begin{enumerate}
    \item compute the residuals of the ``slow`` waveforms from the ``fast`` ones, and with these build a dataset;
    \item train a machine learning system on this dataset, so that it is able to reconstruct the map from the parameters \(\vec{\theta}\) to the residuals.
\end{enumerate}

If this task is accomplished with tolerable errors and with a fast enough execution time (shorter than the time taken by the ``slow'' generation method) we will have a working \emph{surrogate model}. 

The generation of datasets needs us to define a probability distribution over the parameter space: the density of points in each region will bias the reconstruction, but this might actually be desirable if we need to sample from certain regions more than others. 
A good choice, therefore, could be to generate the training dataset according to a prior distribution over the parameters. 

The implementation currently active in \ac{mb}, however, simply generates waveforms with a uniform distribution over the selected parameter ranges: \(q \in [1, 2]\), \(\Lambda _i \in [5, 5000]\)\footnote{The reason why \(\Lambda \to 0\) is not included in the range is that the \ac{EOB} model used is unstable very small \(\Lambda \), since certain computations include divisions by it.} and \(\chi_i \in [-0.5, 0.5]\).

The nonspinning case, which will be discussed later, simply amounts to restricting the prior to the three-dimensional subspace \(\chi _i \equiv 0\).

\subsection{Training waveform generation and downsampling}



\subsection{Phase unwrapping} \label{sec:unwrapping}

The waveforms generated by the \ac{EOB} system are complex-valued, in the form \(h_(f) = A(f) e^{i \phi (f)}\).

It is useful for the later stages of this algorithm to decompose such a waveform into \(A(f)\) and \(\phi (f)\), however in doing one runs into a complication.
The amplitude can be simply calculated as \(A(f) = \abs{h(f)}\), but the phase computed as \(\angle h(f)\) is bounded (between \(-\pi \) and \( \pi \) in the \texttt{numpy} implementation), therefore it is discontinuous when \(h(f)\) crosses the negative real axis. 

In order to overcome this issue we need to make some assumptions about the waveform: we ask that the phase \(\phi (f)\) is smoothly varying, densely sampled and almost monotonic. 

The first two conditions, in practice, refer to the fact that the variation of the phase between two sample points should be small compared to \(2 \pi \) --- otherwise, we would not be able to tell whether it increased by \(x\) or \(2 \pi n + x\) for some integer \(n\). 

If \(\phi (f)\) were very densely sampled, such that \(\Delta \phi \) between two successive points were always \(< \pi \), we could drop the condition of almost-monotonicity: an algorithm could simply compute \(\angle h(f)\) for each sample point, and add \(2 \pi n\) as needed in order to make the differences between successive points \(< \pi \). 
This is the algorithm implemented, for example, by the \href{https://numpy.org/doc/stable/reference/generated/numpy.unwrap.html}{\texttt{unwrap}} function in the \texttt{numpy} library \cite{harrisArrayProgrammingNumPy2020}.

We can do slightly better, however, by using the quasi-monotonicity assumption and treating decreasing phase differently than increasing phase: we add \(2 \pi n\) to each point, determining \(n\) so that \(\phi _{i+1} - \phi _i \in [- \epsilon , 2 \pi - \epsilon ]\), where \(\epsilon \) is some small number quantifying the maximum decrease in phase we expect to see.
If \(\epsilon = \pi \) this is equivalent to the \texttt{numpy} algorithm. 

The monotonicity and magnitude of the phase are not invariant: we can add a general linear term \(2 \pi f t_0 + \phi_0 \) to \(\phi \) by changing the initial phase and the coalescence time. 
However, within the convention we are using (where the coalescence happens at the edge of the sample range) the phase of \(h(f)\) is indeed almost monotonic, such that \(\epsilon \sim 10^{-1}\) is typically a good choice.


\subsubsection{Residual calculation} \label{sec:residuals}


\subsubsection{Greedy downsampling}

The residual waveforms generated with the \ac{EOB} / \ac{PN} models have on the order of a few times \num{e5} sampling points. 
This is too large a number to effectively use \ac{PCA} on: as we will see, it requires writing a covariance matrix which would have \(\sim \num{e11}\) entries, way beyond the \ac{RAM} of most computers. 

Therefore, we need to start by applying a simple dimensionality reduction technique: downsampling, applied to the unwrapped phase and amplitude of waveforms (see section \ref{sec:unwrapping}) after the subtraction of the ``baseline'' \ac{PN} ones.

Doing so on a uniform grid, however, is suboptimal: there is more detail to be captured in some areas than in others. 
It is difficult to determine \emph{a priori} which areas will be more relevant than others, therefore \ac{mb} implements a greedy algorithm which selects a set of indices which optimize the reconstruction accuracy, inspired by \texttt{romspline}, which was introduced by
\textcite{galleyFastEfficientEvaluation2016}. 

The algorithm is detailed as algorithm \ref{alg:greedy-downsampling}; it includes certain improvements over \texttt{romspline}. 
First, it allows for a full training dataset as opposed to reconstruction of a single waveform. The points are greedily chosen until the error over the whole dataset is below the threshold.
This allows us to be more confident of the fact that the indices are not capturing the specifics of a single waveform, but instead they are representative of the whole dataset.

Whether this is actually happening 

The interpolation method to choose is also important: higher-order interpolants are slower but need less points.
Here we use cubic splines, which have been found to be a good compromise for surrogate models \cite{lackeySurrogateModelAlignedspin2019}.


\begin{algorithm}
\caption{Greedy downsampling algorithm. }\label{alg:greedy-downsampling}
\begin{algorithmic}
\Require \(x_i\), \(y_i^{(j)}\), tolerance \(\epsilon \).
\Require ``Seed'' indices \(I\)
\State Error \(e \gets 0\)
\While{\(e > \epsilon \)}
\State \(e \gets \infty \)
\For{each \(j\)}
\State \(I' \gets \emptyset\)
\State \( e_j \gets 0\)
\State \(y_{\text{rec}, i}^{(j)} \gets\) interpolation of \(x_I\), \(y_I^{(j)}\)
\For{\(k_1, k_2\) successive pair in \(I\)}
\State \(n \gets \operatorname{argmax}_{i \in [k_1 , k_2 ]} \ \abs{y _{\text{rec}, i}^{(j)} - y_{i}^{(j)}}\)
\State \(e_j \gets \max \left( e_j, \abs{y _{\text{rec}, n}^{(j)} - y_{n}^{(j)}}\right)\)
\If{\(\abs{y _{\text{rec}, n} - y_{n}^{(j)}} > \epsilon \)}
\State \(I' \gets I' \cup n\)
\EndIf
\State \(I \gets I \cup I'\)
\EndFor
\State \(e \gets \min (e, e_j)\)
\EndFor
\EndWhile \\
\Return indices \(I\)
\end{algorithmic}
\end{algorithm}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/downsampling_validation}
\caption{Validation errors for downsampled waveforms. Downsampling indices are calculated by applying algorithm \ref{alg:greedy-downsampling} to a dataset of varying size, and the error is calculated by computing the maximum reconstruction error for each of \(N _{\text{val}} = 128\) waveforms. Note that ``average'' and ``maximum'' here refer to the difference between taking an average or a maximum over the array of maximum reconstruction errors. }
\label{fig:downsampling_validation}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/downsampling_npoints}
\caption{Size of the downsampling index array deriving from the application of algorithm \ref{alg:greedy-downsampling} to datasets of varying sizes.}
\label{fig:downsampling_npoints}
\end{figure}




\end{document}