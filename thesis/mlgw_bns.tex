\documentclass[main.tex]{subfiles}
\begin{document}

\section{MLGW-BNS}

Let us start with an overview of the algorithm employed by \ac{mb} before getting into the technical details. 

It is based on the use of two systems for the generation of a theoretical waveform from a \ac{CBC} in the frequency domain, one being faster but less accurate than the other.
The ``fast'' system will be the \ac{PN} model \texttt{TaylorF2}, as implemented in \texttt{bajes} \cite[]{breschiTtBajesBayesian2021}, while the ``slow'' system will be the \ac{EOB} model \texttt{TEOBResumS} .\footnote{The wave}

As we shall see shortly, a crucial feature of the ``fast'' system is the capability to generate waveforms in the frequency domain at select frequencies, without needing to perform a full Fourier transform from a time-domain waveform.
This feature is shared by all \ac{PN} approximants, which base their Fourier-domain representations on the analytic \ac{SPA}.

The main idea is then to 
\begin{enumerate}
    \item compute the residuals of the ``slow`` waveforms from the ``fast`` ones, and with these build a dataset;
    \item train a machine learning system on this dataset, so that it is able to reconstruct the map from the parameters \(\vec{\theta}\) to the residuals.
\end{enumerate}

If this task is accomplished with tolerable errors and with a fast enough execution time (shorter than the time taken by the ``slow'' generation method) we will have a working \emph{surrogate model}. 

The generation of datasets needs us to define a probability distribution over the parameter space: the density of points in each region will bias the reconstruction, but this might actually be desirable if we need to sample from certain regions more than others. 
A good choice, therefore, could be to generate the training dataset according to a prior distribution over the parameters. 

The implementation currently active in \ac{mb}, however, simply generates waveforms with a uniform distribution over the selected parameter ranges: \(q \in [1, 2]\), \(\Lambda _i \in [5, 5000]\)\footnote{The reason why \(\Lambda \to 0\) is not included in the range is that the \ac{EOB} model used is unstable very small \(\Lambda \), since certain computations include divisions by it.} and \(\chi_i \in [-0.5, 0.5]\).

The nonspinning case, which will be discussed later as a simplification for which the reconstruction errors improve significantly, simply amounts to restricting the prior to the three-dimensional subspace \(\chi _i \equiv 0\). 
The set

\subsection{Training waveform generation}

The \ac{EOB} and \ac{PN} models we use are able to output both \(h_{+}\) and \(h_{\times }\), but this is redundant for our purposes: the ratio between them is a \(\pi /2\) phase combined with a factor depending on the inclination angle \(\iota \): the two polarizations can be expressed as \(h_{+} (f) = h_0 (f) (1 + \cos^2 \iota ) / 2\) and \(h_{\times }( f) = h_0 (f) (-i) \cos \iota \). 
The ``base'' waveform \(h_0 (f)\) can therefore be interpreted as the plus-polarized waveform in the \(\iota = 0\) case. 

If we reconstruct it, we can recover \(h_{+}\) and \(h_{\times }\) for arbitrary inclination using the two aforementioned expressions.

\subsection{Phase unwrapping} \label{sec:unwrapping}

The waveforms generated by the \ac{EOB} system are complex-valued, in the form \(h(f) = A(f) e^{i \phi (f)}\).

It is useful for the later stages of this algorithm to decompose such a waveform into \(A(f)\) and \(\phi (f)\), however in doing one runs into a complication.
The amplitude can be simply calculated as \(A(f) = \abs{h(f)}\), but the phase computed as \(\angle h(f)\) is bounded (between \(-\pi \) and \( \pi \) in the \texttt{numpy} implementation), therefore it is discontinuous when \(h(f)\) crosses the negative real axis. 

In order to overcome this issue we need to make some assumptions about the waveform: we ask that the phase \(\phi (f)\) is smoothly varying, densely sampled and almost monotonic. 

The first two conditions, in practice, refer to the fact that the variation of the phase between two sample points should be small compared to \(2 \pi \) --- otherwise, we would not be able to tell whether it increased by \(x\) or \(2 \pi n + x\) for some integer \(n\). 

If \(\phi (f)\) were very densely sampled, such that \(\Delta \phi \) between two successive points were always \(< \pi \), we could drop the condition of almost-monotonicity: an algorithm could simply compute \(\angle h(f)\) for each sample point, and add \(2 \pi n\) as needed in order to make the differences between successive points \(< \pi \). 
This is the algorithm implemented, for example, by the \href{https://numpy.org/doc/stable/reference/generated/numpy.unwrap.html}{\texttt{unwrap}} function in the \texttt{numpy} library \cite{harrisArrayProgrammingNumPy2020}.

We can do slightly better, however, by using the quasi-monotonicity assumption and treating decreasing phase differently than increasing phase: we add \(2 \pi n\) to each point, determining \(n\) so that \(\phi _{i+1} - \phi _i \in [- \epsilon , 2 \pi - \epsilon ]\), where \(\epsilon \) is some small number quantifying the maximum decrease in phase we expect to see.
If \(\epsilon = \pi \) this is equivalent to the \texttt{numpy} algorithm. 

The monotonicity and magnitude of the phase are not invariant: we can add a general linear term \(2 \pi f t_0 + \phi_0 \) to \(\phi \) by changing the initial phase and the coalescence time. 
However, within the convention we are using (where the coalescence happens at the edge of the sample range) the phase of \(h(f)\) is indeed almost monotonic, such that \(\epsilon \sim 10^{-1}\) is typically a good choice.

Since this implementation is custom-made it is also slightly faster: it takes \(\sim \SI{30}{ms}\) to unwrap a waveform with \(\sim \num{5e5}\) sampling points,  as opposed to the \(\sim \SI{40}{ms}\) needed for the \texttt{numpy} implementation. 

\subsubsection{Residual calculation} \label{sec:residuals}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/amp_residuals}
\caption{Residuals in amplitude of the EOB waveforms versus the \ac{PN} ones. The waveforms are uniformly distributed across all five parameters. }
\label{fig:amp_residuals}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/phase_residuals}
\caption{Residuals in phase of the \ac{EOB} waveforms versus the \ac{PN} ones. The waveforms are uniformly distributed across all five parameters.
A linear term has been subtracted from all of these for the plot: this does not affect the physical waveform (it only amounts to a time shift), and it is needed because the time-alignment of the \ac{EOB} waveform is not exact.}
\label{fig:phase_residuals}
\end{figure}

The \ac{mb} model reconstructs the residuals of \ac{EOB} waveforms from \ac{PN} ones: these are specifically expressed as 
%
\begin{align} \label{eq:amplitude-phase-residuals}
\Delta A(f;\theta ) &= \log \qty(\frac{A _{\text{EOB}} (f;\theta )}{A _{\text{PN}} (f;\theta )}) \\
\Delta \Phi (f; \theta ) &= \Phi _{\text{EOB}} (f; \theta ) - \Phi _{\text{PN}} (f; \theta )
\,,
\end{align}
%
where \(A\) and \(\Phi \) are the amplitude and phase of the respective \ac{FD} waveforms. 

The computation of the residuals is made easy by the fact that we have closed-form expressions for the \ac{PN} waveforms to high order, including tidal effects.

Using higher-order waveforms means that the residuals to fit are smaller, but this comes with a large drawback: the evaluation time of the \ac{PN} waveforms drastically rises with the order --- they are complicated analytic waveforms. 

As we will discuss later, the reconstruction time for a single waveform 
is dominated by the recovery of the full waveform from the residuals: the computation of \(A _{\text{PN}}(f; \theta )\) and \(\Phi _{\text{PN}}(f; \theta )\) in
%
\begin{align}\label{eq:amplitude-phase-reconstruction}
A _{\text{rec}} (f; \theta ) &= \exp( \Delta A _{\text{rec}} (f; \theta )) A _{\text{PN}}(f; \theta )  \\
\Phi _{\text{rec}} (f; \theta ) &= \Phi _{\text{rec}}(f; \theta ) + \Phi _{\text{PN}}(f; \theta )
\,.
\end{align}

Therefore, a relatively low-order \ac{PN} approximant is chosen; specifically, the amplitude is computed to 3.5PN order, and the phase is computed to 3.5PN order including tidal contributions to 6PN order \cite{favataSystematicParameterErrors2014}.\footnote{Specifically, the functions used are \texttt{Af3hPN} and \texttt{Phif3hPN} \href{https://github.com/matteobreschi/bajes/blob/stable/v0.1/bajes/obs/gw/approx/taylorf2.py}{implemented in the \texttt{bajes} module} \cite[]{breschiTtBajesBayesian2021}.}

\subsection{The reconstruction of a waveform}

We describe the procedure taken by a trained \ac{mb} model in order to reconstruct a waveform starting from a set of parameters \(\theta = [q, \Lambda_1 , \Lambda_2 , \chi_1 , \chi_2 ]\), extrinsic parameters (for simplicity we consider only \(\theta _{\text{ext}} = [M, D_L, \iota]\)), as well as a set of new frequencies \(f\) to re-interpolate the waveform at.

\begin{enumerate}
    \item The parameters \(\theta \) are reduced to \(\theta _r\) by a scaler, which is trained so that they each have zero mean and unit variance. 
    \item The trained \ac{NN} makes its prediction based on the parameters, let us denote it as \(\qty(y_i)_{i=0}^{K-1}\).
    \item The prediction is divided by the corresponding \ac{PCA} eigenvalues, to get \(x_i = y_i / \lambda _i^{\alpha }\).
    \item The \ac{PCA} model reconstructs the normalized, combined vector representing the amplitude and phase residuals: \(\text{ncomb}_i\). 
    \item The normalized, combined vector is de-normalized by multiplying the phase part by the chosen normalization factor \(\sim 15\).
    \item The combined vector is split into the amplitude and phase residual vectors, corresponding to their respective amplitude and phase downsampled frequencies.
    \item The frequencies \(f\) are converted into natural units by dividing them by \(c^3 / GM \approx (M_{\odot} / M) \times  \SI{2e5}{Hz}\); this yields a set of natural-units frequencies \(f_n\).
    \item The residual amplitude and phase are resampled to the frequencies \(f_n\).
    \item The amplitude and phase are reconstructed from the residuals according to equations \eqref{eq:amplitude-phase-reconstruction}.
    \item The waveform is recombined as \(h = A e^{i \Phi }\).
    \item The waveform is rescaled according to \(M\) and \(D_L\), so that it is expressed in SI units (seconds).
    \item The two polarizations are recovered by multiplying \(h\) by their respective functions of \(\iota \) and a phase shift by \(\pi /2\) for \(h_ \times \). 
\end{enumerate}

\todo[inline]{Add new benchmarks}

\subsection{Evaluation time} \label{sec:evaluation-time}

In broad strokes, the evaluation time of a waveform by \ac{mb} is comparable if not slightly better than that of the model it is trained on, \texttt{TEOBResumS} with \ac{SPA}. 
The benchmarks for the evaluation time are shown in figure \ref{fig:profiling_by_f0}, and the ratios between the times for the three models are shown in more detail in figure \ref{fig:profiling_by_f0_ratios}. 

The assumption underlying all these benchmarks is that, as the initial frequency decreases and the duration of the waveform increases as shown in figure \ref{fig:waveform_length}, the number of samples for the Fourier transform must roughly match the number of samples in the time domain; specifically, if \(r\) is the sample rate and \(T\) is the length of the time-series then in the time domain we will need \(rT\) samples to represent the signal. Typically, for \ac{BNS} analysis, the sample rate is set to \(2^{12} \SI{}{Hz} = \SI{4096}{Hz}\), while the duration of the signal depends on the initial frequency we want to consider --- for example, a \ac{BNS} with \(M = 2.8 M_{\odot}\) starting at \SI{12}{Hz} can be described within a time series lasting \SI{1024}{s}, leading to \(2^{22}\) samples.


\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/profiling_by_f0}
\caption{Evaluation times. The frequencies at which the models are evaluated are chosen so that they each correspond to a doubling of waveform duration.}
\label{fig:profiling_by_f0}
\end{figure}

On the other hand, in the frequency domain the standard approach is to use uniform frequency sampling, where the lowest frequency we can describe, \(\Delta f = 1/T\), will determine the sampling rate. 
The highest frequency it is useful to record is the Nyquist frequency \(f_N = r / 2\).  
Since our signal is real-valued, negative frequency samples are redundant --- they are the conjugates of the corresponding positive frequency ones. 
We also do not need to record components at frequencies lower than \(f_0 \), the initial frequency of the signal. 



\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/profiling_by_f0_ratios}
\caption{Ratios of evaluation times: these are the same data as in figure \ref{fig:evaluation_time_by_fsize}, viewed as ratios between the two approximants and \ac{mb}. }
\label{fig:profiling_by_f0_ratios}
\end{figure}

\subsection{Performance} \label{sec:performance}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/mismatches_spinning}
\caption{Mismatches between reconstructed and \ac{EOB} waveforms in the spinning case; we also show the mismatches between the \ac{EOB} waveforms and the original \ac{PN} ones (i.e.\ we set the residuals to zero). We can see that there is a typical improvement in fidelity by a factor \(\sim 10^2\), but the faithfulness is still not good as required.}
\label{fig:mismatches_spinning}
\end{figure}

\subsubsection{Target accuracy}

What is the fidelity \(\mathcal{F}\) we should aim for? 
One criterion to use for a surrogate model such as this one is to look at the accuracy of the underlying model: comparing the \ac{EOB} model to numerical relativity simulations we get typical values for the fidelity of \(\mathcal{F} \simeq \num{2.5e-3}\) \cite[]{nagarTimedomainEffectiveonebodyGravitational2018}.

We can also reframe the issue in terms of signal-to-noise ratio: as discussed e.g.\ by \textcite[]{lindblomModelWaveformAccuracy2008} or \textcite[]{gambaFastFaithfulFrequencydomain2020}, if we are analyzing a signal detected with a certain \ac{SNR} two theoretical waveforms are faithful (indistinguishable for the purposes of the analysis) when 
%
\begin{align}
\mathcal{F} \leq \frac{D}{\text{SNR}^2}
\,,
\end{align}
%
where \(D\) is the number of intrinsic parameters. 
As an example, GW170817 was detected with a combined SNR\(\approx 32.4\) \cite[]{abbottGW170817ObservationGravitational2017}, meaning that the fidelity needed to analyze it is \(\mathcal{F} \lesssim \num{5e-3}\) if we are considering five intrinsic parameters.

\end{document}