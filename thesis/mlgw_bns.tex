\documentclass[main.tex]{subfiles}
\begin{document}

% \chapter{MLGW\_BNS}
\chapter{Machine learning for gravitational waves from neutron star mergers} \label{chap:mlgw-bns}

\begin{figure}[ht]
\centering
\input{flowchart.tex}
\caption{Flowchart for \ac{mb}.}
\label{fig:flowchart}
\end{figure}

\newpage

Let us start with an overview of the algorithm employed by \ac{mb} before getting into the technical details. 

It is based on the use of two systems for the generation of a theoretical waveform from a \ac{CBC} in the frequency domain, one being faster but less accurate than the other.
The ``fast'' system is the \ac{PN} model \texttt{TaylorF2}, as implemented in \texttt{bajes} \cite[]{breschiTtBajesBayesian2021}, while the ``slow'' system is the \ac{EOB} model \texttt{TEOBResumS}. 

As we shall see shortly, a crucial feature of the ``fast'' system is the capability to generate waveforms in the frequency domain at select frequencies, without needing to perform a full Fourier transform from a time-domain waveform, and to do so yielding amplitude and phase separately .
This feature is shared by all \ac{PN} approximants, which base their Fourier-domain representations on the analytic \ac{SPA}.

As illustrated in figure \ref{fig:flowchart}, the main idea is then to 
\begin{enumerate}
    \item compute the residuals of the ``slow`` waveforms from the ``fast`` ones, and with these build a dataset;
    \item train a machine learning system on this dataset, so that it is able to reconstruct the map from the parameters \(\vec{\theta}\) to the residuals;
    \item generate new waveforms by making a prediction for the residuals 
\end{enumerate}

If this task is accomplished with tolerable errors and with a fast enough execution time (shorter than the time taken by the ``slow'' generation method) we will have a working \emph{surrogate model}. 

The generation of datasets needs us to define a probability distribution over the parameter space: the density of points in each region will bias the reconstruction, but this might actually be desirable if we need to sample from certain regions more than others. 

A good choice, therefore, could be to generate the training dataset according to a nonuniform distribution over the parameters --- this might be not be an astrophysically motivated prior: ideally, we would want it to resolve regions in which the reconstruction is most difficult with more points.

The implementation currently active in \ac{mb}, however, simply generates waveforms with a uniform distribution over the selected parameter ranges: \(q \in [1, 2]\), \(\Lambda _i \in [5, 5000]\)\footnote{The reason why \(\Lambda \to 0\) is not included in the range is that the \ac{EOB} model used is unstable for very small \(\Lambda \), since certain computations include divisions by it.} and \(\chi_i \in [-0.5, 0.5]\).

As discussed in section \ref{sec:natural-units}, these are all the parameters needed for the reconstruction: the dependence on the total mass \(M\), on the inclination \(\iota \) and on the luminosity distance \(d_L\) can be recovered analytically after the natural-units waveform has been constructed.  

% The nonspinning case, which will be discussed later as a simplification for which the reconstruction errors improve significantly, simply amounts to restricting the prior to the three-dimensional subspace \(\chi _i \equiv 0\). 
% The set

\section{Training dataset generation}

A \ac{mb} model requires several independent training datasets. These are each computed by generating random parameters with differently seeded \acp{PRNG}, according to the aforementioned uniform distribution.

This is all accomplished with a single call of the \texttt{generate\_dataset} method of the \texttt{Model} class in \ac{mb}.

The first dataset needed is the one for the training of the greedy downsampling algorithm (see section \ref{sec:downsampling}). 
These are unwrapped (section \ref{sec:unwrapping}) \ac{EOB} waveforms, sampled at full resolution. 
The downsampling algorithm yields a set of frequencies for the amplitude and one for the phase, which are saved and kept constant for that model, such that interpolating waveforms sampled at these frequencies with a cubic spline does not introduce errors beyond a certain, small threshold.

Two more datasets are then generated with the same technique: 
downsampling the \ac{EOB} waveforms generated at new pseudo-random parameter values, computing their residuals (see section \ref{sec:residuals}) from the \ac{PN} waveforms, and recording these. 

The first of these datasets is then used to train a \ac{PCA} model, while the second is kept to serve as training, validation and testing data for the \ac{NN}.

The generation of these last two datasets is the longest part of the procedure, but it is parallelized, so making a dataset with \(f_0 \sim \SI{20}{Hz}\) and \(\sim 1024\) waveforms in both can take as little as a few minutes.
This time can get significantly longer as \(f_0 \) decreases. 

\subsection{Phase unwrapping} \label{sec:unwrapping}

The waveforms generated by the \ac{EOB} system are complex-valued, in the form \(h(f) = A(f) e^{i \phi (f)}\).

It is useful for the later stages of this algorithm to decompose such a waveform into \(A(f)\) and \(\phi (f)\), however in doing one runs into a complication.
The amplitude can be simply calculated as \(A(f) = \abs{h(f)}\), but the phase computed as \(\angle h(f) = \arctan (\Im h / \Re h)\) is bounded (between \(-\pi \) and \( \pi \) in the \texttt{numpy} implementation), therefore it is discontinuous when \(h(f)\) crosses the negative real axis. 

In order to overcome this issue we need to make some assumptions about the waveform: we ask that the phase \(\phi (f)\) is smoothly varying, densely sampled and almost monotonic. 

The first two conditions, in practice, refer to the fact that the variation of the phase between two sample points should be small compared to \(2 \pi \) --- otherwise, we would not be able to tell whether it increased by \(x\) or \(2 \pi n + x\) for some integer \(n\). 

If \(\phi (f)\) were very densely sampled, such that \(\Delta \phi \) between two successive points were always \(< \pi \), we could drop the condition of almost-monotonicity: an algorithm could simply compute \(\angle h(f)\) for each sample point, and add \(2 \pi n\) as needed in order to make the differences between successive points \(< \pi \). 
This is the algorithm implemented, for example, by the \href{https://numpy.org/doc/stable/reference/generated/numpy.unwrap.html}{\texttt{unwrap}} function in the \texttt{numpy} library \cite{harrisArrayProgrammingNumPy2020}.

We can do slightly better, however, by using the quasi-monotonicity assumption and treating decreasing phase differently than increasing phase: we add \(2 \pi n\) to each point, determining \(n\) so that \(\phi _{i+1} - \phi _i \in [- \epsilon , 2 \pi - \epsilon ]\), where \(\epsilon \) is some small number quantifying the maximum decrease in phase we expect to see.
If \(\epsilon = \pi \) this is equivalent to the \texttt{numpy} algorithm. 

The monotonicity and magnitude of the phase are not invariant: we can add a general linear term \(2 \pi f t_0 + \phi_0 \) to \(\phi \) by changing the initial phase and the coalescence time. 
However, within the convention we are using (where the coalescence happens at the edge of the sample range) the phase of \(h(f)\) is indeed almost monotonic, such that \(\epsilon \sim 10^{-1}\) is typically a good choice.

Since this implementation is custom-made it is also slightly faster: it takes \(\sim \SI{30}{ms}\) to unwrap a waveform with \(\sim \num{5e5}\) sampling points,  as opposed to the \(\sim \SI{40}{ms}\) needed for the \texttt{numpy} implementation. 

\subsection{Residual calculation} \label{sec:residuals}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/amp_residuals}
\caption{Residuals in amplitude of the EOB waveforms versus the \ac{PN} ones. The waveforms are uniformly distributed across all five parameters. }
\label{fig:amp_residuals}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/phase_residuals}
\caption{Residuals in phase of the \ac{EOB} waveforms versus the \ac{PN} ones. The waveforms are uniformly distributed across all five parameters.
A linear term has been subtracted from all of these for the plot: this does not affect the physical waveform (it only amounts to a time shift), and it is needed because the time-alignment of the \ac{EOB} waveform is not exact.}
\label{fig:phase_residuals}
\end{figure}

The \ac{mb} model reconstructs the residuals of \ac{EOB} waveforms from \ac{PN} ones: these are specifically expressed as 
%
\begin{subequations}
\begin{align} \label{eq:amplitude-phase-residuals}
\Delta A(f;\theta ) &= \log \qty(\frac{A _{\text{EOB}} (f;\theta )}{A _{\text{PN}} (f;\theta )}) \\
\Delta \Phi (f; \theta ) &= \Phi _{\text{EOB}} (f; \theta ) - \Phi _{\text{PN}} (f; \theta )
\,,
\end{align}
\end{subequations}
%
where \(A\) and \(\Phi \) are the amplitude and phase of the respective \ac{FD} waveforms. 

The computation of the residuals is made easy by the fact that we have closed-form expressions for the \ac{PN} waveforms to high order, including tidal effects.

The highest-order available \ac{PN} approximant is chosen; specifically, the amplitude is computed to 3.5PN order, and the phase is computed to 5.5PN order including tidal contributions to 7.5PN order \cite{favataSystematicParameterErrors2014}.
Specifically, the functions used are \texttt{Af3hPN} and \texttt{Phif5hPN} \href{https://github.com/matteobreschi/bajes/blob/stable/v0.1/bajes/obs/gw/approx/taylorf2.py}{implemented in the \texttt{bajes} module} \cite[]{breschiTtBajesBayesian2021}. Their evaluation is accelerated thanks to just-in-time compilation with \texttt{numba} \cite[]{lamNumbaLLVMbasedPython2015}.

The waveforms produced by the two systems are not perfectly aligned in the time domain, so the dominant contribution to the residuals is the linear term due to the phase difference. This can be safely removed from all the phases (see the discussion on \ref{sec:arrival-time}). 
An arbitrary choice must be made for this normalization: since we know the low-frequency behaviours of the two approximants should match, we impose that the phase residuals and their derivative at the low-frequency end of the spectrum both equal zero.

After the \ac{PCA}+\ac{NN} ensemble reconstructs the residuals, the waveform can be recovered as 
%
\begin{subequations}
\begin{align}\label{eq:amplitude-phase-reconstruction}
A _{\text{rec}} (f; \theta ) &= \exp( \Delta A _{\text{rec}} (f; \theta )) A _{\text{PN}}(f; \theta )  \\
\Phi _{\text{rec}} (f; \theta ) &= \Phi _{\text{rec}}(f; \theta ) + \Phi _{\text{PN}}(f; \theta )
\,.
\end{align}
\end{subequations}

Because of the subtraction of the linear term from the phase, the reconstructed waveform will be time-shifted by some small amount compared to the corresponding \ac{EOB} one. 

\subsection{PCA for waveforms}

After downsampling and computing the residuals from the \ac{PN} waveform, we are left with a waveform described by a vector of a few thousand points for the amplitude and similarly for the phase. 

It would be possible to perform \ac{PCA} (described in section \ref{sec:principal-component-analysis}) separately for these two vectors; however if we combine them into a single one we can exploit any existing correlation between the amplitude and phase residuals. 
In the worst case scenario --- amplitude and phase residuals being completely uncorrelated --- this procedure will perform exactly like separating the \ac{PCA} into two. 
So, we want to combine amplitude and phase residuals into a single vector. 

A simple way to do so is to simply ``append'' one vector to the other, and therefore consider the waveform as a vector in \(\mathbb{R}^{D_A + D_\varphi }\). 

The mismatches resulting from this procedure are shown in figure \ref{fig:PCA_vary_K}.

\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{figures/PCA_vary_K}
\caption{The reconstruction error is computed after projecting onto a \(K\)-dimensional subspace a number \(N_w=64\) waveforms. The parameters for the \ac{PCA} model are fitted with an independent, \(N _{\text{PCA}} = 128\) waveform dataset. A \ac{KDE} is performed for each set of corresponding log-fidelities, in order for the probability curve to look smoother.
Here the \ac{EOB} waveforms are calculated with a \ac{SPA}. 
A vertical dashed line indicates a ``desirable'' maximal reconstruction error, \(\mathcal{F} \sim \num{e-3}\), which roughly corresponds to the accuracy of the underlying \ac{EOB} model to \ac{NR} simulations.
}
\label{fig:PCA_vary_K}
\end{figure}

\subsection{Optimization}

Tuning the hyperparameters with \texttt{optuna} as described in section \ref{sec:optuna} is the longest step in the preparation of a \ac{mb} model. 
Therefore, a fiducial set of hyperparameters is provided as a constant, and it is used by default if the user does not wish to wait for the optimization. 

Otherwise, the method \texttt{optimize\_hyper} for the \texttt{Model} class can be called. 
The following network parameters are varied during the optimization procedure:\footnote{Most of these are parameters in the initialization of an \texttt{MLPRegressor} from \texttt{scikit-learn} \cite{pedregosaScikitlearnMachineLearning2011}.}
\begin{enumerate}
    \item the number of layers is varied between 2 and 4, and the sizes of each layer are separately varied between 0 and 100;
    \item the activation function is varied between a \ac{ReLU} \(x \to \max{0, x}\), a hyperbolic tangent \(x 
\to \tanh(x)\), and a logistic function \(x \to (1 + e^{-x})^{-1}\);
    \item the L2 regularization term \(\alpha \) is varied with a log-uniform distribution in \([10^{-6}, 10^{-1}]\);
    \item the batch size for the stochastic optimization (see section \ref{sec:sgd}) is varied with a uniform distribution in \([150, 250]\);
    \item the optimization is stopped when the performance does not improve by \texttt{tol} for more than \texttt{n\_iter\_no\_change} iterations --- \texttt{tol} is varied with a log-uniform distribution in \([10^{-15}, 10^{-7}]\), while \texttt{n\_iter\_no\_change} is varied with an integer-valued log-uniform distribution in \([40, 100]\);
    \item the exponent for the \ac{PCA} eigenvalues multiplying the \ac{PCA} components for the training data is varied with a log-uniform distribution in \([10^{-3}, 1]\);
    \item the training dataset \emph{size} can be varied with an integer-valued log-uniform distribution between 100 and the total data available.
\end{enumerate}

We do not optimize the \ac{PCA} parameter at this stage --- that part of the optimization is done independently, setting a cautiously high value of \(K_{\text{PCA}} = 40\) which, as discussed in section \ref{sec:principal-component-analysis}, is  enough to reconstruct waveforms with fidelity \(\mathcal{F} \lesssim 10^{-6}\). 

Varying the training dataset size may seem peculiar: should we not always train the network with all the data available? 
If the dataset size is not too large this can work, but with larger datasets (say, larger than \(10^4\) waveforms) the training time for a single network can become quite long, on the order of several minutes. 
This is not a problem if we only need to train it once, but the optimization procedure requires several hundred or even thousands of training instances, so it quickly becomes very inefficient if each time we use a huge dataset. 

On the other hand, we cannot assume that the optimal set of hyperparameters is independent of the size of the training dataset: heuristically, we should expect a network which is able to capture small-scale features to be prone to overfitting on a small dataset, while it might perform well with a large dataset. 

Allowing for the training on smaller datasets can allow us to explore the hyperparameter space in a faster way.
We have found no significant improvement beyond dataset sizes of the order \(\sim 10^3\), which is consistent with the results of \textcite{schmidtMachineLearningGravitational2020}.

The optimization is carried out on two fronts with a \ac{MOTPE}: one cost function is determined by the accuracy of the reconstruction, while the other is determined by the time of the generation of the training dataset, plus the training of the \ac{NN}. 

The accuracy of the reconstruction is computed on a validation dataset, which is independent of the training data for the \ac{NN} (see section \ref{sec:dataset-management}). 
Ideally, we would want to evaluate the mismatches, but this is not computationally efficient; instead, we use the surrogate measurement 
%
\begin{align}
\text{error} =\expval{ \frac{1}{\# i_\Phi } \sum _{i \in i_\Phi } \qty(\Delta \Phi ^{\text{rec}}_i - \Delta \Phi ^{\text{EOB-PN}}_i )^2 + \frac{1}{\# i_A} \sum _{i \in i_A} \qty(\Delta \log A ^{\text{rec}}_i - \Delta \log A ^{\text{EOB-PN}}_i)^2}
\,,
\end{align}
%
where the average is taken over the validation dataset, while the index sets \(i_\Phi \) and \(i_A\) are the results of the greedy downsampling algorithm (see section \ref{sec:downsampling}), and \(\# S\) denotes the cardinality of set \(S\). 

The evaluation time is computed by timing the \ac{NN} training, and then adding a penalty for the training dataset size, heuristically set to \SI{100}{ms} per training waveform. 

% hyperparameter optimization
% which hyperparameters?
% dependence on training dataset size

\subsection{FFT versus SPA}

The use of the \ac{SPA} is common in \ac{GW} data analysis because of its speed, but it is still an approximation: should we not use a full \ac{FFT} of the \ac{EOB} waveforms? 

The mismatch between \ac{SPA} and \ac{FFT} waveforms generated by \texttt{TEOBResumS} was studied by \textcite{gambaFastFaithfulFrequencydomain2020}, who found that they are typically on the order of \(\mathcal{F} \approx \num{e-4}\), with maximal values on the order of \(\approx \num{5e-4}\). 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/FFT-noise-lowf}
\caption{Comparison between the amplitude for waveforms generated from the same \ac{EOB} model, moved to the frequency domain with either the \ac{SPA} or a \ac{FFT}. The \ac{FFT} is noisy due to windowing effects.}
\label{fig:FFT-noise-lowf}
\end{figure}

This is reassuring if we want to use the \ac{SPA}: the error it introduces is an order of magnitude lower than that of the model itself. 

Why would we want to use it though, if it just introduces an error?
One reason is speed: transforming a waveform with a \ac{FFT} is quite slow, and it quickly becomes the main bottleneck in the generation of the training dataset.

The second and more important reason is the windowing noise in the computation of the transform.
Figure \ref{fig:FFT-noise-lowf} shows an example of what it looks like: since the frequency resolution is quite fine, it is able to resolve the difference between frequencies ``constructively or destructively interfering'' with the time-domain window in which we are computing the waveform. 
This results in small fluctuations in the amplitude. These are not a large concern by themselves, but they are large enough to prevent the greedy downsampling algorithm (section \ref{alg:greedy-downsampling}) from working properly, since it assumes that the amplitude, like the phase, is smoothly varying. 

Because of these two aspects, \ac{mb} reconstructs smooth \ac{SPA} waveforms. It is a prospect for the future to improve it by including \ac{FFT}-informed corrections to the high-frequency region, in which the physically meaningful difference between the \ac{SPA} and the \ac{FFT} manifests. 

\section{The reconstruction of a waveform}

We describe the procedure taken by a trained \ac{mb} model in order to reconstruct a waveform starting from a set of intrinsic parameters \(\theta = [q, \Lambda_1 , \Lambda_2 , \chi_1 , \chi_2 ]\) and extrinsic parameters \(\theta _{\text{ext}} = [M, D_L, \iota]\), as well as a set of arbitrary new frequencies \(f\) to re-interpolate the waveform at. 
This is the procedure denoted as ``prediction'' in figure \ref{fig:flowchart}.

\begin{enumerate}
    \item The parameters \(\theta \) are reduced to \(\theta _r\) by a scaler, which is trained so that they each have zero mean and unit variance. \label{item:param-reduction}
    \item The trained \ac{NN} makes its prediction based on the parameters, let us denote it as \(\qty(y_i)_{i=0}^{K-1}\). \label{item:nn-prediction}
    \item The prediction is divided by the corresponding \ac{PCA} eigenvalues, to get \(x_i = y_i / \lambda _i^{\alpha }\).
    \item The \ac{PCA} model reconstructs combined vector representing the amplitude and phase residuals: \(\text{comb}_i\). \label{item:pca-reconstruction}
    % \item The normalized, combined vector is de-normalized by multiplying the phase part by the chosen normalization factor \(\sim 15\).
    \item The combined vector is split into the amplitude and phase residual vectors, corresponding to their respective amplitude and phase downsampled frequencies. \label{item:combined-split}
    \item The frequencies \(f\) are converted into natural units by dividing them by \(c^3 / GM \approx (M_{\odot} / M) \times  \SI{2e5}{Hz}\); this yields a set of natural-units frequencies \(f_n\). \label{item:frequency-conversion}
    \item The amplitude and phase are reconstructed from the residuals according to equations \eqref{eq:amplitude-phase-reconstruction}. \label{item:tf2}
    \item The amplitude and phase are resampled to the frequencies \(f_n\). \label{item:resample}
    \item The waveform is recombined as \(h = A e^{i \Phi }\). \label{item:recombine}
    \item The waveform is rescaled according to \(M\) and \(D_L\), so that it is expressed in SI units (seconds). \label{item:rescale}
    \item The two polarizations are recovered by multiplying \(h\) by their respective functions of \(\iota \) and a phase shift by \(\pi /2\) for \(h_ \times \). \label{item:polarizations}
\end{enumerate}

As we shall discuss in section \ref{sec:evaluation-time}, the second-slowest part of the reconstruction for a waveform is the inclusion of extrinsic parameters, in points \ref{item:rescale} and \ref{item:polarizations}. 
Therefore, these have been sped up with just-in-time compilation with \texttt{numba} \cite{lamNumbaLLVMbasedPython2015}.

\subsection{Evaluation time} \label{sec:evaluation-time}

In broad strokes, on standard benchmarks the evaluation time of a waveform by \ac{mb} is slightly better than that of the model it is trained on, \texttt{TEOBResumS} with \ac{SPA}; while in simpler but realistic scenarios its evaluation time is an order of magnitude faster or more.

The standard benchmarks for the evaluation time are shown in figure \ref{fig:profiling_by_f0}, and the ratios between the times for the three models are shown in more detail in figure \ref{fig:profiling_by_f0_ratios}. 

The assumption underlying all these benchmarks is that, as the initial frequency decreases and the duration of the waveform increases (as shown in figure \ref{fig:waveform_length}), the number of samples for the Fourier transform is determined by the \ac{FFT} grid, so it must roughly match the number of samples in the time domain; specifically, if \(r\) is the sample rate and \(T\) is the length of the time-series then in the time domain we will need \(rT\) samples to represent the signal. 
Typically, for \ac{BNS} analysis, the sample rate is set to \(2^{12} \SI{}{Hz} = \SI{4096}{Hz}\), while the duration of the signal depends on the initial frequency we want to consider --- for example, a \ac{BNS} with \(M = 2.8 M_{\odot}\) starting at \SI{12}{Hz} can be described within a time series lasting \SI{1024}{s}, leading to \(2^{22}\) samples in the time domain.


\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/profiling_by_f0}
\caption{Evaluation times. The frequencies at which the models are evaluated are chosen so that they each correspond to a doubling of waveform duration.}
\label{fig:profiling_by_f0}
\end{figure}

On the other hand, in the frequency domain the standard approach is to use uniform frequency sampling, where the lowest frequency we can describe, \(\Delta f = 1/T\), will determine the sampling rate. 
The highest frequency it is useful to record is the Nyquist frequency \(f_N = r / 2\).  
Since our signal is real-valued, negative frequency samples are redundant --- they are the conjugates of the corresponding positive frequency ones. 
We also do not need to record components at frequencies lower than \(f_0 \), the initial frequency of the signal. 
Therefore, the total number of frequency samples is 
%
\begin{align}
N = \frac{f_N - f_0 }{\Delta f} +1 = \qty( \frac{r}{2} - f_0 )T + 1 \approx \frac{r}{2} T
\marginnote{We sample both endpoints, hence the \(+1\).}
\,.
\end{align}

In the aforementioned case, this corresponds to roughly \(2^{21} \approx \num{2.1e6}\) samples. In terms of storage, each of these samples is a complex number, typically represented with two 64-bit floating point Cartesian components, therefore a waveform represented in this way will take up \SI{32}{MiB} of memory.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/profiling_by_f0_ratios}
\caption{Ratios of evaluation times: these are the same data as in figure \ref{fig:profiling_by_f0}, viewed as ratios between the two approximants and \ac{mb}. }
\label{fig:profiling_by_f0_ratios}
\end{figure}

Neither \ac{mb} nor \texttt{TEOBResumS} natively work with so many points, and both need to interpolate their results on a fine grid such as this one. 
This is the reason why \ac{mb}, as shown in figure \ref{fig:profiling_by_f0_ratios}, is only slightly faster than \texttt{TEOBResumS} when the initial frequency is very low: both algorithms are bottle-necked by resampling, which can be performed in linear time with respect to the number of points in the new, densely sampled array. 

\subsection{Downsampled waveform evaluation time} \label{sec:downsampled-evaluation}

In order to illustrate this point, let us discuss the breakdown of an evaluation of \ac{mb} in three different cases.
In all three it reconstructs both polarizations for a \ac{BNS} waveform with \(f_0 = \SI{12}{Hz}\), with a time-domain sampling rate of \SI{4096}{Hz}, total mass \(M = 2.8M_{\odot}\), mass ratio \(q \approx 1.96\), aligned spins \(\chi_{1z} \approx 0.38\) and \(\chi_{2z} \approx 0.04\), tidal deformabilities \(\Lambda_1 \approx 3600 \) and \(\Lambda_2 \approx 2500\), luminosity distance \(d_L = \SI{25}{Mpc}\), and inclination \(\iota = \SI{3}{rad}\).

The times we give refer to a single evaluation of \ac{mb}, so they cannot be considered fully representative, but they were found to be fairly consistent among different runs; the most significant difference between different runs was found to be the overall scaling of the times, which depends on system factors such as other processes running and memory availability, while the ratios were generally preserved.

\paragraph{Full resampling}
Here the waveform is resampled to the full number of points, which equals \((f_N - f_0 ) / \Delta f + 1 = 2084865 \approx \num{2e6}\). 
The total time taken by \ac{mb} is \SI{385}{ms},\footnote{This is slightly faster than the typical time shown in figure \ref{fig:profiling_by_f0} for an initial frequency of \SI{12}{Hz}: this comes down to the state of the system at the time the benchmarks were taken --- the data for the figure was computed at a different time than the following benchmarks, but the overall slowdown seems to preserve ratios, so the speedups we discuss are rather consistent.
Still, the breakdown in the following section refers to a single trial, so it should be taken with a grain of salt, while the benchmarks in the figures are the results of the averages of several trials.} of which 
\begin{enumerate}
    \item \SI{246}{ms} (\SI{69}{\%}) are taken by the interpolation (point \ref{item:resample});
    \item \SI{103}{ms} (\SI{27}{\%}) are taken by extrinsic parameter operations (points \ref{item:recombine} through \ref{item:polarizations});
    \item \SI{0.9}{ms} (\SI{0.2}{\%}) are taken by the \ac{NN} + \ac{PCA} routines (points \ref{item:param-reduction} through \ref{item:combined-split});
    \item \SI{0.5}{ms} (\SI{0.15}{\%}) are taken by the evaluation of the \ac{PN} waveform (point \ref{item:recombine});
    \item the rest of the time (\SI{16}{ms}, \SI{4}{\%}) is taken by other post-processing operations (such as point \ref{item:frequency-conversion}).
\end{enumerate}

The evaluation time of the baseline \ac{PN} model, as well as the \ac{PCA} and \ac{NN} steps, are basically negligible. 

In a comparable scenario, \texttt{TEOBResumSPA} took \SI{452}{ms}, or around \SI{17}{\%} more. 

\paragraph{16-fold downsampling}

Here the waveform provided by \ac{mb} is evaluated on a grid made 16 times sparser, so consisting of 130305 points.
These were taken to be still equally spaced for simplicity, even though it is not a realistic usage case: the \ac{mb} model can interpolate to an arbitrary frequency array, and providing an efficiently chosen, non-uniform frequency array does not change its performance. 

In this case the total time taken by \ac{mb} is \SI{34}{ms}, of which 
\begin{enumerate}
    \item \SI{22}{ms} (\SI{64}{\%}) are taken by the interpolation (point \ref{item:resample});
    \item \SI{10}{ms} (\SI{30}{\%}) are taken by extrinsic parameter operations (points \ref{item:recombine} through \ref{item:polarizations});
    \item \SI{0.6}{ms} (\SI{1.8}{\%}) are taken by the \ac{NN} + \ac{PCA} routines (points \ref{item:param-reduction} through \ref{item:combined-split});
    \item \SI{0.6}{ms} (\SI{1.7}{\%}) are taken by the evaluation of the \ac{PN} waveform (point \ref{item:recombine});
    \item the rest of the time (\SI{0.7}{ms}, \SI{2}{\%}) is taken by other post-processing operations (such as point \ref{item:frequency-conversion}).
\end{enumerate}

In the same scenario, \texttt{TEOBResumSPA} took \SI{98}{ms}, or around three times more. 

\paragraph{256-fold downsampling}

Here the waveform provided by \ac{mb} is evaluated on a grid made 256 (\( = 16^2\)) times sparser, so consisting of 8145 points.

In this case the total time taken by \ac{mb} is \SI{4.2}{ms}.
Here we give a more detailed breakdown of the times taken by some subroutines, since they can be more clearly interpreted as percentages of this total. 

\begin{enumerate}
    \item \SI{2}{ms} (\SI{49}{\%}) are taken by the interpolation (point \ref{item:resample});
    \begin{enumerate}
        \item of these, \SI{0.7}{ms} (\SI{16}{\%} of the total) are taken by the creation of the cubic spline --- this absolute time is consistent, regardless of the number of points it is then evaluated on; 
        \item \SI{1.4}{ms} (\SI{33}{\%} of the total) are taken by the evaluation of the spline.
    \end{enumerate}
    \item \SI{0.8}{ms} (\SI{19}{\%}) are taken by the \ac{NN} + \ac{PCA} routines (points \ref{item:param-reduction} through \ref{item:combined-split});
    \begin{enumerate}
        \item of these, \SI{0.6}{ms} (\SI{15}{\%} of the total) are taken by the \ac{NN} prediction (point \ref{item:nn-prediction}), while
        \item \SI{0.1}{ms} (\SI{3}{\%} of the total) are taken by the \ac{PCA} reconstruction (point \ref{item:pca-reconstruction});
    \end{enumerate}
    \item \SI{0.6}{ms} (\SI{15}{\%}) are taken by extrinsic parameter operations (points \ref{item:recombine} through \ref{item:polarizations});
    \item \SI{0.5}{ms} (\SI{13}{\%}) are taken by the evaluation of the \ac{PN} waveform (point \ref{item:recombine});
    \item the rest of the time (\SI{0.1}{ms}, \SI{3}{\%}) is taken by other post-processing operations (such as point \ref{item:frequency-conversion}).
\end{enumerate}

In the same scenario, \texttt{TEOBResumSPA} typically takes \SI{70}{ms}, or around 15 times more than \ac{mb}. 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/profiling_by_ds}
\caption{Evaluation time for waveforms starting at \SI{12}{Hz}, as detailed in section \ref{sec:downsampled-evaluation}; at the very left of the graph the waveforms are evaluated at roughly two million points, while at the very right they are evaluated at roughly one thousand points.}
\label{fig:profiling_by_ds}
\end{figure}

\subsection{Reduced order modelling} \label{sec:reduced-order-modelling}

The improvement given by \ac{mb} as opposed to the \ac{EOB} model \texttt{TEOBResumS} is shown in more detail in figure \ref{fig:profiling_by_ds}: if the waveform is sampled at fewer points, \ac{mb} yields a significant speedup. 

How many points do we actually need to sample at? Not that many! 
\textcite{vinciguerraAcceleratingGravitationalWave2017}, for example, outline a multi-banding approach: the chirping behavior of the signal means that we know \emph{a priori} that the content at high frequencies is only relative to the short section of time relative to the end of the timeseries. 

They subdivide the frequency series into bands with varying frequency resolution, but they keep the resolution constant within each band.
This allows for a reduction of the number of points in frequency space by a factor of roughly 150 for \(f_0 = \SI{12}{Hz}\) (the same value as that used in figure \ref{fig:profiling_by_ds}), with very low, mismatches of a few times \num{e-7}. 

This already would allow for millisecond-scale \ac{mb} evaluations. 

Alternative techniques for fast likelihood evaluation exist: an example is \ac{ROQ} \cites{smithFastAccurateInference2016}{canizaresAcceleratedGravitationalWave2015}, in which a basis of \(N\) waveforms is constructed (typically through a greedy algorithm), among which scalar products can be pre-computed; then, waveforms can be represented in this basis by sampling them at \(N\) specific points. 
The size of these bases typically ranges from \(10^3\) to \(10^4\). 

Another technique is relative binning \cite{zackayRelativeBinningFast2018}, in which ratios of the frequency domain waveform to a fiducial, less precise one are downsampled. 

% Ideas from all of these are applied throughout \ac{mb}, which 

\subsection{Accuracy} \label{sec:accuracy}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/MLGW_BNS_vs_TF2}
\caption{Mismatches between reconstructed and \ac{EOB} waveforms in the spinning case; we also show the mismatches between the \ac{EOB} waveforms and the original \ac{PN} ones (i.e.\ we set the residuals to zero). All waveforms are reconstructed starting from \SI{12}{Hz}, and the mismatches are computed according to the \ac{aLIGO} \ac{PSD}.}
\label{fig:MLGW_BNS_vs_TF2}
\end{figure}

Section \ref{sec:target-fidelity} discussed some order-of-magnitude estimates for the fidelity required for matched filtering and for parameter estimation. 

Figure \ref{fig:MLGW_BNS_vs_TF2} shows the mismatches between the waveforms reconstructed by \ac{mb} and the reference ones generated by \texttt{TEOBResumS}. 

Since the algorithm is based on \ac{PN} waveforms, we also show the corresponding mismatches computed between the \ac{PN} model and the \ac{EOB} one. 
We can clearly see an improvement by a factor 10 or more --- the histograms barely overlap, and the \ac{mb} has a much heavier tail into very low \(\mathcal{F}\). 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/recon-amp-residuals}
\caption{Log-amplitude reconstruction residuals for waveforms starting at \SI{12}{Hz}. }
\label{fig:recon-amp-residuals}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{figures/recon-phase-residuals}
\caption{Phase reconstruction residuals for waveforms starting at \SI{12}{Hz}. A linear fit is subtracted from all of these; this is arbitrary, however it shows the magnitude of the phase residuals for a close-to-optimal time alignment.}
\label{fig:recon-phase-residuals}
\end{figure}

The corresponding amplitude and phase residuals are shown in figures \ref{fig:recon-amp-residuals} and \ref{fig:recon-phase-residuals}. 

\begin{figure}[ht]
\centering
\includegraphics[width=.9\textwidth]{figures/MLGW_BNS_reconstruction_varying_PSD}
\caption{\ac{KDE} plot of the mismatches between the same waveforms reconstructed by \ac{mb} as figure \ref{fig:MLGW_BNS_vs_TF2}, varying the \ac{PSD}: we use the \ac{aLIGO} and \ac{ET} \acp{PSD}, as well as a uniform \(S_n \equiv \const\) for reference. }
\label{fig:MLGW_BNS_reconstruction_varying_PSD}
\end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\textwidth]{figures/TF2_reconstruction_varying_PSD}
% \caption{\ac{KDE} plot of the mismatches between the same waveforms reconstructed by \ac{mb} as figure \ref{fig:MLGW_BNS_vs_TF2}, varying the \ac{PSD}: we use the \ac{aLIGO} and \ac{ET} \acp{PSD}, as well as a uniform \(S_n \equiv \const\) for reference. }
% \label{fig:TF2_reconstruction_varying_PSD}
% \end{figure}

Figure \ref{fig:MLGW_BNS_reconstruction_varying_PSD} shows how the mismatches computed with the \ac{aLIGO} \ac{PSD} change if we use another one. 
Specifically, they are almost unchanged with the predicted \ac{ET} \ac{PSD}, while they are lower with a reference, unphysical constant noise \ac{PSD}.

% Remove this bit?

\chapter{Conclusions and outlook}

% \textcite{cuocoEnhancingGravitationalwaveScience2020}

In this work we developed the surrogate gravitational waveform model \ac{mb}.
It natively works in the frequency domain, and it is able to generate five-parameter waveforms in only a few milliseconds if these do not need to be sampled at more than about \num{e4} points --- as can be seen in figure \ref{sec:downsampled-evaluation}, this represents a 15-fold improvement with respect to the reference frequency-domain model, \texttt{TEOBResumS} with a \ac{SPA}.

The fidelity of the reconstructed waveforms to the ones in the training dataset is, in the worst case, comparable to the intrinsic accuracy of the \ac{EOB} waveforms themselves. 
This is an acceptable level for current interferometers (see section \ref{sec:target-fidelity}) but it is still not enough for third generation detectors such as \ac{ET}. 
An improvement in the reconstruction accuracy of \ac{mb} should accompany improvements in the underlying \ac{PN} and \ac{EOB} models.

A surrogate such as \ac{mb} can be thought of as a plugin: it can improve the evaluation time of any ``slow'' model, as long as we are able to generate a large enough dataset of waveforms with it. 

A full-scale test of the system in the context of parameter estimation should be performed, ideally combining it with one of the reduced-order modelling techniques discussed in section \ref{sec:reduced-order-modelling}.
This would allow for a demonstration of its speed, as well as a validation of its accuracy.

Currently, \ac{mb} only reconstructs the \((2, 2)\) mode of gravitational radiation: a path for improvement is the reconstruction of other modes, which would allow for a proper treatment of precessing \ac{BNS} systems (i.e.\ ones with arbitrary spin orientations). 

Another aspect which could be optimized in view of future detectors relates to the low-frequency regime: there, the \ac{PN} approximation is quite accurate, as can be seen in figures \ref{fig:amp_residuals} and \ref{fig:phase_residuals}. 
Thus, there is a point beyond which evaluating the \ac{EOB} model and computing its residuals from the \ac{PN} one is superfluous. 
Implementing this is trickier than simply appending zeroes to the start of the residual vectors, since we must ensure a smooth connection --- while they approach zero they do not reach it exactly. 
Nevertheless, if this can be implemented it can allow the \ac{mb} model to be trained on waveforms which are faster to generate, while being able to make predictions up to arbitrarily low frequencies.
This is particularly relevant in the context of future, third generation \ac{GW} detectors such as \ac{ET}.

% A similar idea to this is to 
% precession / modes
% smoothly connecting to TF2
% publishing as a package
% trying different PN models and training datasets

\end{document}
